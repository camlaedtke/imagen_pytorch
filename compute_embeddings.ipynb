{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4b394a1",
   "metadata": {},
   "source": [
    "# Computation of caption text embeddings\n",
    "\n",
    "To do\n",
    "- Iron out bugs in the batch downloader\n",
    "- Use [dask delayed](https://docs.dask.org/en/stable/delayed.html). Configure scheduling so that GPU computations wait until a few hundred captions are accumilated. Then run computation, take out individual embeddings, truncate, and write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba3742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import braceexpand\n",
    "from tqdm import tqdm\n",
    "from imagen_pytorch.t5 import t5_encode_text\n",
    "import webdataset as wds\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce26df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_augment_wds(input, output, batch_size=256):\n",
    "    \n",
    "    stats_file = input[:-4] + \"_stats.json\"\n",
    "    f = open(stats_file)\n",
    "    stats = json.load(f)\n",
    "    f.close()\n",
    "    count = stats[\"successes\"]\n",
    "    \n",
    "    src = wds.DataPipeline(\n",
    "        wds.SimpleShardList(input),\n",
    "        wds.tarfile_to_samples(),\n",
    "        wds.decode(\"pil\"),\n",
    "        wds.to_tuple(\"__key__\", \"jpg;png\", \"txt\"),\n",
    "        wds.map_tuple(None, None, None)\n",
    "    )\n",
    "    \n",
    "    dst = wds.TarWriter(output) \n",
    "    \n",
    "    keys = []; imgs = []; caps = []\n",
    "    for idx, (key, img, cap) in enumerate(src):\n",
    "        keys.append(key)\n",
    "        imgs.append(img)\n",
    "        caps.append(cap)\n",
    "        if ((idx+1)%batch_size == 0) or idx==count:\n",
    "            print(f\"\\r Step {idx}/{count}\", end='')\n",
    "            batch_embeds = t5_encode_text(caps, name=\"google/t5-v1_1-xl\", return_attn_mask=False)\n",
    "            batch_embeds = batch_embeds.cpu() # consider removing \n",
    "            embs = []\n",
    "            for tensor in batch_embeds:\n",
    "                ix, iy = tensor.nonzero(as_tuple=True)\n",
    "                tensor_nonzero = tensor[0:max(ix), :]\n",
    "                embs.append(tensor_nonzero)\n",
    "            for key_, img_, cap_, emb_ in zip(keys, imgs, caps, embs):\n",
    "                dst.write({\"__key__\":key_, \"png\":img_, \"txt\":cap_, \"emb.pyd\":emb_})\n",
    "            keys = []; imgs = []; caps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b3add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_tensor(text):\n",
    "    text_embeds = t5_encode_text([text], name=\"google/t5-v1_1-xl\", return_attn_mask=False)\n",
    "    return text_embeds.cpu()\n",
    "\n",
    "\n",
    "def augment_wds(input, output):\n",
    "    \n",
    "    stats_file = input[:-4] + \"_stats.json\"\n",
    "    f = open(stats_file)\n",
    "    stats = json.load(f)\n",
    "    f.close()\n",
    "    count = stats[\"successes\"]\n",
    "    \n",
    "    src = wds.DataPipeline(\n",
    "        wds.SimpleShardList(input),\n",
    "        wds.tarfile_to_samples(),\n",
    "        wds.decode(\"pil\"),\n",
    "        wds.to_tuple(\"__key__\", \"jpg;png\", \"txt\", \"txt\"),\n",
    "        wds.map_tuple(None, None, None, get_emb_tensor)\n",
    "    )\n",
    "    \n",
    "    with wds.TarWriter(output) as dst:\n",
    "        for key, img, cap, emb in tqdm(src, total=count, desc=f\"Writing {output}\"):\n",
    "            dst.write({\"__key__\":key, \"png\":img, \"txt\":cap, \"emb.pyd\":emb})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287196c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_shards = braceexpand.braceexpand(\"cc12m/{00400..01242}.tar\")\n",
    "output_shards = braceexpand.braceexpand(\"file:E:/datasets/cc12m/{00400..01242}.tar\")\n",
    "for input_shard, output_shard in zip(input_shards, output_shards):\n",
    "    augment_wds(input=input_shard, output=output_shard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402cfb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shards = braceexpand.braceexpand(\"cc12m/{00154..00200}.tar\")\n",
    "# output_shards = braceexpand.braceexpand(\"cc12m_aug/{00154..00200}.tar\")\n",
    "# for input_shard, output_shard in zip(input_shards, output_shards):\n",
    "#     batch_augment_wds(input=input_shard, output=output_shard, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4cde28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shards = braceexpand.braceexpand(\"cc12m/{00100..00102}.tar\")\n",
    "# output_shards = braceexpand.braceexpand(\"file:E:/datasets/cc12m/{00100..00102}.tar\")\n",
    "# results = []\n",
    "# for input_shard, output_shard in zip(input_shards, output_shards):\n",
    "#     results.append(dask.delayed(augment_wds)(input_shard, output_shard))\n",
    "# dask.compute(*results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
