{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fce36fc",
   "metadata": {},
   "source": [
    "# Imagen Training Script on CocoCaptions Dataset\n",
    "\n",
    "CocoCaptions Dataset - [documentation](https://pytorch.org/vision/main/generated/torchvision.datasets.CocoCaptions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a431fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import yaml\n",
    "import math\n",
    "import wandb\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "from time import time\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from flatdict import FlatDict\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, utils\n",
    "from imagen_pytorch.t5 import t5_encode_text\n",
    "from imagen_pytorch import Unet, Imagen, ImagenTrainer, ElucidatedImagen\n",
    "from utils.data_utils import CocoDataset\n",
    "from utils.train_utils import get_emb_tensor, display_images, save_checkpoint, print_epoch_stats\n",
    "\n",
    "logging.propagate = False \n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae09c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = yaml.safe_load(Path(\"configs\\\\imagen-config.yaml\").read_text())\n",
    "cfg_flat = dict(FlatDict(cfg, delimiter='.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd28de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe5941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"imagen\", entity=\"camlaedtke\", config=cfg_flat)#, resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e82d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dataset = CocoDataset(cfg)\n",
    "    \n",
    "coco_dataloader = DataLoader(\n",
    "    dataset = coco_dataset, \n",
    "    batch_size = cfg[\"train\"][\"batch_size\"], \n",
    "    shuffle = True,\n",
    "    drop_last = True,\n",
    "    num_workers = 4,\n",
    "    prefetch_factor = 8,\n",
    "    pin_memory = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cccc8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch, y_batch = next(iter(coco_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb45fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "display_data(X_batch[0:6], y_batch[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fbc259",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MODEL #####\n",
    "BaseUnet = Unet(\n",
    "    dim = cfg[\"model\"][\"base_unet\"][\"dim\"],\n",
    "    cond_dim = cfg[\"model\"][\"base_unet\"][\"cond_dim\"],\n",
    "    dim_mults = cfg[\"model\"][\"base_unet\"]['dim_mults'], \n",
    "    num_resnet_blocks = cfg[\"model\"][\"base_unet\"][\"num_resnet_blocks\"],\n",
    "    layer_attns = cfg[\"model\"][\"base_unet\"]['layer_attns'], \n",
    "    layer_cross_attns = cfg[\"model\"][\"base_unet\"]['layer_cross_attns'], \n",
    "    attn_heads = cfg[\"model\"][\"base_unet\"][\"attn_heads\"],\n",
    "    ff_mult = cfg[\"model\"][\"base_unet\"][\"ff_mult\"],\n",
    "    memory_efficient = cfg[\"model\"][\"base_unet\"][\"memory_efficient\"],\n",
    "    dropout = cfg[\"model\"][\"base_unet\"][\"dropout\"]\n",
    ")\n",
    "\n",
    "\n",
    "SRUnet = Unet(\n",
    "    dim = cfg[\"model\"][\"sr_unet1\"][\"dim\"],\n",
    "    cond_dim = cfg[\"model\"][\"sr_unet1\"][\"cond_dim\"],\n",
    "    dim_mults = cfg[\"model\"][\"sr_unet1\"][\"dim_mults\"], \n",
    "    num_resnet_blocks = cfg[\"model\"][\"sr_unet1\"][\"num_resnet_blocks\"], \n",
    "    layer_attns = cfg[\"model\"][\"sr_unet1\"][\"layer_attns\"],\n",
    "    layer_cross_attns = cfg[\"model\"][\"sr_unet1\"][\"layer_cross_attns\"], \n",
    "    attn_heads = cfg[\"model\"][\"sr_unet1\"][\"attn_heads\"],\n",
    "    ff_mult = cfg[\"model\"][\"sr_unet1\"][\"ff_mult\"],\n",
    "    memory_efficient = cfg[\"model\"][\"sr_unet1\"][\"memory_efficient\"],\n",
    "    dropout = cfg[\"model\"][\"sr_unet1\"][\"dropout\"]\n",
    ")\n",
    "\n",
    "\n",
    "imagen = Imagen(\n",
    "    unets = (BaseUnet, SRUnet),\n",
    "    text_encoder_name = cfg[\"model\"][\"text_encoder_name\"], \n",
    "    image_sizes = cfg[\"model\"][\"image_sizes\"], \n",
    "    cond_drop_prob = cfg[\"model\"][\"cond_drop_prob\"],\n",
    "    timesteps = cfg[\"model\"][\"timesteps\"],\n",
    ").cuda()\n",
    "\n",
    "##### TRAINING #####\n",
    "trainer = ImagenTrainer(\n",
    "    imagen, \n",
    "    lr = cfg[\"train\"][\"lr\"],\n",
    "    amp = cfg[\"train\"][\"amp\"],\n",
    "    use_ema = cfg[\"train\"][\"use_ema\"],\n",
    "    warmup_steps = eval(cfg[\"train\"][\"warmup_steps\"]),\n",
    "    cosine_decay_max_steps = eval(cfg[\"train\"][\"cosine_decay_max_steps\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f56b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg, dataloader, trainer, epoch, i, device):\n",
    "    loss_arr = []\n",
    "    fetch_times = []; embed_times = []; loss_times = []; update_times = []; step_times = []\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        step_start = time()\n",
    "        \n",
    "        fetch_start = time()\n",
    "        images, texts = batch\n",
    "        images = images.to(device)\n",
    "        fetch_end = time()\n",
    "        fetch_times.append(fetch_end-fetch_start)\n",
    "        \n",
    "        embed_start = time()\n",
    "        text_embeds, text_masks = get_emb_tensor(cfg, texts, device)\n",
    "        embed_end = time()\n",
    "        embed_times.append(embed_end-embed_start)\n",
    "\n",
    "        loss_start = time()\n",
    "        loss = trainer(\n",
    "            images, \n",
    "            text_embeds = text_embeds, \n",
    "            text_masks = text_masks, \n",
    "            unet_number = i, \n",
    "            max_batch_size=cfg[\"train\"][\"base_unet_max_batch_size\"] if i==1 else cfg[\"train\"][\"sr_unet1_max_batch_size\"]\n",
    "        )\n",
    "        loss_end = time()\n",
    "        loss_times.append(loss_end-loss_start)\n",
    "        \n",
    "        update_start = time()\n",
    "        trainer.update(unet_number = i)\n",
    "        update_end = time()\n",
    "        update_times.append(update_end-update_start)\n",
    "        \n",
    "        step_end = time()\n",
    "        step_times.append(step_end-step_start)\n",
    "        \n",
    "        \n",
    "        loss_arr.append(loss)\n",
    "        save_checkpoint(cfg, step, loss, trainer)\n",
    "        \n",
    "        curr_step = int(len(dataloader)*(epoch-1) + step)\n",
    "        wandb.log({f\"Train Loss {i}\": loss, f\"Train {i} Step\": curr_step})\n",
    "        print(f\"\\r   Train Step {step+1}/{len(train_dataloader)}, Train Loss: {loss:.4f}\", end='')\n",
    "        \n",
    "    \n",
    "    step_time = np.mean(step_times)\n",
    "    fetch_time = np.mean(fetch_times)\n",
    "    embed_time = np.mean(embed_times)\n",
    "    loss_time = np.mean(loss_times)\n",
    "    update_time = np.mean(update_times)\n",
    "    print()\n",
    "    print(f\"      Step: {step_time:.4f}s, Img load: {fetch_time:.4f}s, Embed: {embed_time:.4f}s, \"\\\n",
    "          f\"Loss: {loss_time:.4f}s, Update: {update_time:.4f}s\")\n",
    "    return trainer, loss_arr\n",
    "\n",
    "\n",
    "\n",
    "def run_train_loop(cfg, trainer, dataloader, device):\n",
    "    \n",
    "    for epoch in range(1, cfg[\"train\"][\"epochs\"]+1):\n",
    "        print(f\"\\nEpoch {epoch}/{cfg['train']['epochs']}\")\n",
    "        \n",
    "        for i in (1,2):\n",
    "            \n",
    "            print(f\"--- Unet {i} ---\")\n",
    "            start = time()\n",
    "\n",
    "            trainer, loss_arr = train(cfg, dataloader, trainer, epoch, i, device)\n",
    "\n",
    "            end = time()\n",
    "            e_time = (end-start)/60 \n",
    "\n",
    "            print_epoch_stats(e_time, loss_arr)\n",
    "            if not math.isnan(loss_arr[-1]): \n",
    "                trainer.save(cfg[\"train\"][\"checkpoint_path\"])\n",
    "            \n",
    "        texts = [\n",
    "            'red flowers in a white vase',\n",
    "            'a puppy looking anxiously at a giant donut on the table',\n",
    "            'the milky way galaxy in the style of monet'\n",
    "        ]\n",
    "        sampled_images = trainer.sample(texts, cond_scale = cfg[\"train\"][\"cond_scale\"])\n",
    "        image_list = display_images(sampled_images)\n",
    "        images_pil = [Image.fromarray(image) for image in image_list]\n",
    "        wandb.log({\"Samples\": [wandb.Image(image) for image in images_pil], \"Epoch\": epoch})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_train_loop(cfg, trainer, train_dataloader, valid_dataloader, device):\n",
    "    \n",
    "    for epoch in range(1, cfg[\"train\"][\"epochs\"]+1):\n",
    "        print(f\"\\nEpoch {epoch}/{cfg['train']['epochs']}\")\n",
    "        \n",
    "        for i in (1,2):\n",
    "            \n",
    "            print(f\"--- Unet {i} ---\")\n",
    "            start = time()\n",
    "\n",
    "            trainer, train_loss_arr = train(cfg, train_dataloader, trainer, epoch, i, device)\n",
    "\n",
    "            valid_loss_arr = [0]\n",
    "            if epoch % 5 == 0:\n",
    "                valid_loss_arr = validate(cfg, valid_dataloader, trainer, epoch, i, device)\n",
    "\n",
    "            end = time()\n",
    "            e_time = (end-start)/60 \n",
    "\n",
    "            print_epoch_stats(e_time, train_loss_arr, valid_loss_arr)\n",
    "            if not math.isnan(valid_loss_arr[-1]): \n",
    "                trainer.save(cfg[\"train\"][\"checkpoint_path\"])\n",
    "            \n",
    "        texts = [\n",
    "            'red flowers in a white vase',\n",
    "            'a puppy looking anxiously at a giant donut on the table',\n",
    "            'the milky way galaxy in the style of monet'\n",
    "        ]\n",
    "        sampled_images = trainer.sample(texts, cond_scale = cfg[\"train\"][\"cond_scale\"])\n",
    "        clear_output()\n",
    "        image_list = display_images(sampled_images)\n",
    "        images_pil = [Image.fromarray(image) for image in image_list]\n",
    "        wandb.log({\"Samples\": [wandb.Image(image) for image in images_pil], \"Epoch\": epoch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e79458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    trainer.load(cfg[\"train\"][\"checkpoint_path\"], strict=False)\n",
    "    print(\"Loaded checkpoint\")\n",
    "except: \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceafa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce43b7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_train_loop(cfg, trainer, train_dataloader, valid_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba29c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'red flowers on a beach by the sunset',\n",
    "    'a puppy looking anxiously at a giant donut on the table',\n",
    "    'the milky way galaxy in the style of monet'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba343cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampled_images = trainer.sample(texts, cond_scale = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872012fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = display_images(sampled_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812711cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
